<html>


<head>
<title>Haskell Exchange Bytes October 2016</title>

<link rel="stylesheet" href="highlight.js/styles/default.css">
<script src="highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link rel="stylesheet" href="Slidy2/styles/slidy.css" type="text/css" />
<script src="Slidy2/scripts/slidy.js" charset="utf-8" type="text/javascript"></script> 
</head>

<body>

<div class="slide">
<pre>

0. TODO Cover slide for screen before talk: a "beautiful destination" picture chosen by neural net?

https://www.instagram.com/beautifuldestinations/

</pre>
</div>
<div class="slide">
<h1>Introduction</h1>
<pre>
Beautiful Destinations - we're doing a lot of data science on images. I'm
not going to talk much about that. More interested in more general topics.

 - overview of three sections that are really only coupled
   by the fact that I have worked on all three on the same
   project
     - shake
     - ghc/ghcjs
     - postgres database subsetting

</pre>

</div>

<div class="slide">
<h1>1. Shake</h1>

<pre>

* describing existing processes in shake

The environment when I arrived:
 * core haskell product built by Makefile and deployed via apt
 * python based projects - history of experimental data analysis (researchy) rather 
   than production code. some of this needed to become production,
   so wanted things like productible builds rather than "well if I have exactly
   my laptop it runs" (not a criticism - its a genuine different in dev
   practice that needs interfacing)
 * other projects appearing ad-hoc in various combinations of languages
   - similar research prototype->production transitions need to be managed
       - even from people who are professional programmers, rather than
         PhDs

Move to scripting this a bit more - ended up with a pile of scripts
but you still had to know what to run and in which order.

</pre>

</div>

<div class="slide">

<pre>
what is shake? it's a library for writing build systems, rather than
a command line tool called "shake".

Note that this isn't intended to be some canonical use of Shake - 
it's how it seemed to me to be a (not the) right way to do it.
</pre>

</div>

<div class="slide">
<pre>
Two uses of shake in this project:

 * the small one
 * the big one
</pre>
</div>

<div class="slide">
<h1>Initial small use</h1>


<pre>
Before:

Makefile based build system, but make doesn't fit
stack built Haskell well at all.

Make is very file-oriented: here is a .c file, and
here is a corresponding .o file.

Or you can have targets which always run - because
Make can't understand the file structure enough. So
most of our Makefile consists of that: dependencies to
run commands in order, but always running them.

Stack manages to not take a lot of time so that bit isn't
too painful.

but post-processing ghcjs output was.

</pre>

<pre>
This takes several minutes:

Something like this: (not quite exact)

<code class="makefile">
frontpack: frontbuild
        (cd $(JSDIR) && rm -f hash_*)
        (cd $(JSDIR) && cat rts.js lib.js out.js > all.js)
        (cd $(JSDIR) && ccjs all.js --warning_level=QUIET \
          --compilation_level=SIMPLE_OPTIMIZATIONS  > all.min.js)
        (cd $(JSDIR) && zopfli -i10 all.min.js)
        (find public/platform/ -name '*.css' -exec cat {} \; \
           > $(JSDIR)/all.css)
        (cd $(JSDIR) && zopfli -i10 all.css)
        $(eval JSHASH := $(shell cd $(JSDIR) && md5sum all.min.js.gz | cut -c1-16))
        echo $(JSDIR)
        ls $(JSDIR)
        echo JSHASH=$(JSHASH)
        (cd $(JSDIR) && cp all.min.js.gz hash_$(JSHASH).js.gz)
        (cd $(JSDIR) && cp all.min.js hash_$(JSHASH).js)

</code>
</pre>

<pre>

What couldn't we express in Make that we can express in shake?
   - we combine *all* the css files in a directory. this doesn't
     look like "here are some input files and an output file"


Maybe shake can help?
 Hadn't used it before - didn't want to embark on rewriting
whole makefile. Almost didn't use shake but was going to start
going interesting scripting to do similar stuff.
</pre>

</div>

<div class="slide">
<h1>Now:</h1>
<pre>
Makefile now buids and calls shake:
<code class="makefile">
frontpack: frontbuild
        (cd $(JSDIR) && rm -fv hash_*)
        stack build shake
        stack exec -- runhaskell shake/BuildMinified.hs -j2 --jsdir=$(JSDIR) ${SHAKEOPTS}
        $(eval JSHASH := $(shell cd $(JSDIR) && md5sum all.min.js.gz | cut -c1-16))
        echo $(JSDIR)
        ls $(JSDIR)
        echo JSHASH=$(JSHASH)
        (cd $(JSDIR) && cp all.min.js.gz hash_$(JSHASH).js.gz)
        (cd $(JSDIR) && cp all.min.js hash_$(JSHASH).js)
</code>
</pre>
</div>

<div class="slide">
<h1>BuildMinified.hs</h1>
<pre>
Now have build rules in BuildMinified.hs

More verbose, programmy rather than scripty.

Show a few excerpts
</pre>

</div>

<div class="slide">
<h1>top level</h1>
<pre><code class="haskell">
  want
         [ jsdir &lt;/> "all.css"
         , jsdir &lt;/> "all.css.gz"
         , jsdir &lt;/> "all.min.js"
         , jsdir &lt;/> "all.min.js.gz"
         ]

  "//*.min.js" %> minify
  "//*.gz" %> compress
  -- NO DEPENDENCIES WRITTEN HERE!

  (jsdir &lt;/> "all.js") %> combineJS jsdir
  (jsdir &lt;/> "all.css") %> combineCSS jsdir 
  -- OR HERE!
</code></pre></div>

<div class="slide">
<h1>helper functions</h1>
<pre><code class="haskell">

combineCSS jsdir outFilename = do
  let cssDir = "public/platform"
  cssFiles <- getDirectoryFiles cssDir ["//*.css"]
  -- DEPENDENCY HERE! dependency isn't a file. Rebuild if list changes
  let fullCssFiles = (cssDir </>) <$> cssFiles
  need fullCssFiles 
  -- DEPENDENCY HERE! files, but list is built at runtime. Rebuild if any change.
  combineFiles fullCssFiles outFilename

compress compressedFilename = do
  let uncompressedFilename = compressedFilename -<.> ""
  need [uncompressedFilename] -- DEPENDENCY ON FILE
  f <- isFast -- DEPENDENCY HERE: This isn't even a disk artefact! Rebuild if flags change. Oracles!
  if f
    then cmd $ "gzip --keep --force -1 " ++ uncompressedFilename
    else cmd $ "zopfli -i10 " ++ uncompressedFilename
</code>
</pre>
</div>

<div class="slide">
<h1>Bigger: Buildosaurus</h1>
<pre>

  *   building "everything".
    on the TODO list to do "something" for a long time, but
     catalyst was new junior sys admin starting and me having
     to explain our processes - I decided I'd rather write it
     down in machine readable form rather than English.

     and made confident by the smaller project...

    what are the kinds of things we're building?
      - debian packages
      - docker images
      - test "certifications" - describe this later

    this only happened in the last few weeks. payoffs so far:
      devs can build "official" style production releases on
      their machines. can re-use parts of CI/production infrastrucutre
      to stand up fuller test environments on their local
      machines.
   part of move towards a mono-repo -  change some docker file
     because we need a different compiler, for example, and
     we can version that together with the code that needs
     the different compiler (the monorepo part) and we can
     build without too much redundancy - this tool is helping us
     do that!

  catch all the bodges that we needed - eg not able to share
    ~/.stack,ghc,ghcjs between two of our components so we
    have to reroute; and makefile isn't expressive enough to
    give single-target builds.
    but now we have a codified place to describe those bodges
     and work on them rather than it being folklore about
      "oh delete your ~/.stack"

  we can start with a machine with docker on it and checkout
   repos, run a single build command, and it all builds (over
    many hours)

   TODO: a graph of all the components. parallelise with 
    modern shake so we can see parallelism? clean build on
    fresh AWS machine so no pollution

  using caches from  other tools: docker cache, .stack cache
    - these have different semantics - eg docker is content
        based, and can cope with multiple versons in cache at
       once, where shake's build directory approach is
         "either you have the current version or its invalid
          and we must rebuild". stack is slightly opaque and
         misbehaving sometimes.
    - don't get to control those so much. (downside)

  - oracles - what oracles implemented? review all

<code class="haskell">
  addOracle getFilteredDirectoryContentsOracle
  addOracle alwaysValidateDockerImageOracle
  addOracle getCommitIDOracle
</code>


  -- built as a regular stack project - multiple source files,
     stack-managed dependencies

   test certifications - track which tests have run (at a
     big-chunk scale - because eg don't stand up a new DB every
     individual test )  
       - test certification file is a file generated to
          represent that the test passed - the "successful" output
          of the tets. imagine an IO operation that returns ()
          or errors. eithre you get the value because it passed
          or you don't.
      - can rebuild on things other than a source code change 
           - eg against latest database snapshot. this is "nothing
          special"  - just a regular dependency.

</pre>
</div>

<div class="slide">
<h1>downsides</h1>

<pre>
 - ops people have to learn "some" Haskell

 - enough changes happening that I still want to
    pay attention to 'master' vs recent releases.

 - need a bit more infrastructure in place (eg stack)
    to build, which is a bit of a chicken-and-egg
    situation because I want buildosaurus to be
    managing things like that.

 - simple stuff feels much more verbose.
   (but complex stuff feels much simpler / actually
    expressable)
</pre>

</div>
<div class="slide">
<h1>2. ghcjs vs ghc</h1>

<ul>
<li>Overall good experience but rough round edges</li>
<li>Shared source code between frontend (in browser) and backend (on server)</li>
<li>... but we've endedup with a lot of <code>#ifdef</code> ickiness</li>
</ul>
</div>
<div class="slide">
<h1>The good: Shared source code</h1>
<ul>
<li>Mostly shared data structures: representing our
domain objects but also queries/commands</li>
<li>Version this as a single application version
(aka git commit ID) running in a distributed setting.</li>
<li>
Data structures shared between frontend and backend
only need to work within that same version - that
is type checked, like other data passed around inside
an application</li>
<li>Communication between halves is HTTP GET/POST + JSON,
but that is not
fundamental: most JSON is done by generics and
we can probably replace that straightforwardly with
any other internal serialisation protocol</li>

</div>

<div class="slide">
<h1>The bad: Rough edges</h1>
<ul>
<li>We run our own patched ghcjs, for Template Haskell
performance. It was unusably slow. <a href="https://github.com/ghcjs/ghcjs/issues/449">#449</a></li>
<li>stack vs ghcjs doesn't version shared state correctly. Workaround in
buildosaurus using Docker magic. <a href="https://github.com/commercialhaskell/stack/issues/2365">#2365</a></li>
<li>Some days I seem to spend my whole day compiling ghcjs over and over</li>
</ul>
</div>


<div class="slide">
<h1>The Ugly: the <code>#ifdef</code> monster</h1>

<p>Our <code>lib/</code> is riddled with
<code>#ifdef</code>.</p>

</div>

<div class="slide">
<h1>Cross platform string types: Text vs JSString</h1>

<pre><code class="haskell">
data Account = Account
  { id              :: AccountId
  , username        :: Text
  } deriving Generic, FromJSON, ToJSON
</code>
</pre>
but...
<pre><code class="haskell">
#ifndef __GHCJS__
import Data.Text (Text)
#else
type Text = JSString
#endif
</code>
</pre>
</div>

<div class="slide">
<h1>The Ugly: the <code>#ifdef</code> monster</h1>
<ul>
<li>Text and other types: Int vs Integer, postgres
structures that leak</li>
<li>Poor simulation of files/modules/separate libraries
    for historical reasons: eg no postgres, so #ifdef
    away anything that talks about postgres on the
    frontend side.</li>
</ul>
</div>
<div class="slide">
<h1>The WTF</h1>
<ul>
<li><code>cabal configure</code> for ghcjs runs in JavaScript.</li>
<li>So in node.js (as does Template Haskell)
<li>To call shell commands for preprocessing use ghcjs FFI from node.js</li>

<pre><code class="haskell">
#ifdef __GHCJS__
import GHCJS.Types (JSString)
import Data.JSString (pack)

foreign import javascript unsafe "var e = require('child_process').execSync;
                                  var cmd = 'cpp -P < ' + $1 + ' > ' + $2; e(cmd);"
  cpp :: JSString -> JSString -> IO ()

main = do
  cpp (pack "jsbits/fast-renderer.js") (pack "jsbits/fast-renderer.out.js")
  defaultMain
#endif

</code>
</pre>
</div>

<div class="slide">
<pre>

         - config/package language not expressive enough (see later)

  - all these ifdefs hark back to writing multiplatform C code when I started as developer
     - they're expressing a thing that the language (haskell or C) doesn't express well
       so we need a DSL on the front of that to generate haskell appropriate for our 
       platform.

  - build/deploy looks different for thge two platforms: in one we generate some exes and run them;
       in the other we generate a bunch of javascript which needs postprocessing and
       webserving. postprocessing, as mentioned in build system section, is a small piece of shake
</pre>

</div>
<div class="slide">
<h1>3. postgres-subset tool / database testing</h1>

<p>Wanted to do database testing in CI.</p>

<p>Haven't come across a tool that did what I wanted
(which surprised me a bit because I don't think it is
 that obscure)</p>

<pre>
 
 - initial driving goal: we want to have a small database in our
   CI that a run can mutate without affecting other CI stuff. (other approaches
   to this are "do some state changes, then tidy yourself up afterwards")

 - our big database takes on the order of 100G to store and hours to save/restore.
    (mostly some indexes) - that's too much for a CI run that we want to happen
    in minutes.

  - tool that didn't do what I want - take a big DB and
     output some subset of it that can be restored fast.
</pre>

</div>
<div class="slide">
<h1>The problem</h1>
<p>
Export a subset of tables based on some configuration. eg recent posts:
</p>

<pre><code class="sql">SELECT * FROM posts WHERE created_at > now() - interval '2 months'
</code></pre>

<p>... but other tables might use <code>posts</code> in a foreign key.</p>
<p>Can't export tables individually - where there is a foreign key, we
only export rows which are still valid given choices made in upstream tables.</p>

</div>

<div class="slide">
<h1>Fixpoints in SQL</h1>

<p>... but that isn't enough, because
tables can have foreign keys on themselves.</p>

<p>eg image table that represents "is thumbnail of" relation</p>

<p>Iterately shrink table until we reach a fixpoint.</p>

<p>SQL has <code><a href="https://www.postgresql.org/docs/9.6/static/queries-with.html">WITH RECURSIVE</a></code> which can find fixpoints when
expanding data, but not when shrinking data. So implemented iteration
in Haskell.</p>

<p>But even this might not be enough (it is for our database though)
because there might be a mutually recursive group of tables, that need
to be iteratively shrunk all together.</p>

<pre>

                            Haskell mdo fixpoint - recursive value but need to grab information from other sources when doing it - from the config file for eg. overridden foreign keys, and from the database itself

</pre>

</div>
<div class="slide">
<h1>use in testing</h1>
<ul>
<li>
what do we need to configure? what parts highlight a "smell" in database
definition? (fake foreign keys? what aren't they defined in the schema?)</li>

<li>giving a different database to the
    app that is not the mega-production-app database. so it should work. but sometimes
    it doesn't. what was wrong? bug in schema definition again? bug in
    application code that is highlighted by being used against a slightly
    different but valid database? </li>

</ul>

</div>
<div class="slide">
<h1>bugs</h1>

<ul>

<li> time related bugs  - SQL clauses are mostly functional, but now() isn't.
      noticed at the point where we had lots of records created very rapidyly
      a month ago - so now() iterating was selecting a different subset every
      time. would have converged eventually. but maybe to 0 and maybe after
      running for a month...

       - pretty straightforward IO vs pure problem 
</li>

<li> race condition between tables being dumped 
      - again, shared state (the database) changing underneath us.
      - SQL has the isolation we need: transactions
</li>

<li> mutually recursive definitions - hopefully straightforward, when
       needed </li>

</div>

<div class="slide">
<h1>fin</h1>
</div>
</body>

</html>
